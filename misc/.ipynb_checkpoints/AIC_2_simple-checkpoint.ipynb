{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59f2fd2-877b-4125-ab49-48191ed88222",
   "metadata": {},
   "source": [
    "A Beginner-Friendly ETL Tutorial with Dummy Datasets\n",
    "Welcome! This comprehensive tutorial will walk you step-by-step through an Extract, Transform, Load (ETL) workflow using four fabricated (dummy) datasets. While the project requirements are analogous to the elevator scenario you described, we’ll use a simpler, more relatable context: car registrations and inspections. This will allow you to practice the same concepts (collecting data, merging, cleaning, filtering, exploratory analysis, etc.) without getting lost in a real, more complex dataset.\n",
    "\n",
    "Table of Contents\n",
    "Introduction\n",
    "Getting Started\n",
    "Dummy Datasets Description\n",
    "1. Registrations Dataset (registration.csv)\n",
    "2. Ownership Dataset (ownership.json)\n",
    "3. Modifications Dataset (modifications.json)\n",
    "4. Testing (Inspection) Dataset (testing.csv)\n",
    "Working Through the ETL Steps\n",
    "Data Extraction: Loading the Data into Pandas\n",
    "Data Exploration & Transformation\n",
    "Part 1 – Analysis of Registrations (registration.csv)\n",
    "1a) Identify Unique Identifier\n",
    "1b) Majority Location & Extract Country/State\n",
    "1c) Filter the DataFrame on Registration Status\n",
    "1d) Verify Uniqueness of the Identifier\n",
    "1e) Time Series of Expiration Counts by Month\n",
    "1f) Create a Year-Month Table with Counts\n",
    "Part 2 – Analysis of Ownership (ownership.json)\n",
    "2a) Merge Filtered Registrations with Ownership\n",
    "2b) Compare Location Fields\n",
    "2c) Further Location Cleanup/Filtering\n",
    "2d) Identify and Apply Filter on a Variable\n",
    "2e) Clean or Reduce Categories of a Variable\n",
    "Part 3 – Analysis of Modifications (modifications.json)\n",
    "3a) Merge with the Already Merged Data\n",
    "3b) Find Cars with More than 5 Modifications\n",
    "3c) Count Combinations of Modification Type and Status\n",
    "Part 4 – Analysis of Testing (testing.csv)\n",
    "4a) Merge Strategy to Avoid Many-to-Many Issues\n",
    "4b) Clean the Inspection Outcome Variable\n",
    "4c) Calculate the Average Number of Inspections per Car\n",
    "Conclusion\n",
    "Introduction\n",
    "In any data-driven project, ETL—Extract, Transform, Load—is an essential sequence:\n",
    "\n",
    "Extract: Gathering data from various sources.\n",
    "Transform: Cleaning, organizing, and reformatting the data to make it usable.\n",
    "Load: Storing the cleaned data in a database or using it for analysis/modeling.\n",
    "We’ll simulate each step with four dummy datasets in CSV/JSON formats. By the end, you’ll have practical experience reading, merging, cleaning, filtering, and visualizing data using Pandas, NumPy, Matplotlib, and basic Python.\n",
    "\n",
    "Getting Started\n",
    "Prerequisites:\n",
    "\n",
    "Good grasp of core Python syntax (loops, functions, data structures).\n",
    "No prior experience with data-focused libraries is strictly required—we’ll explain as we go.\n",
    "Setup:\n",
    "\n",
    "Install Anaconda (which includes Jupyter Notebook, Pandas, NumPy, Matplotlib, etc.).\n",
    "Open Jupyter Notebook (or JupyterLab) and create a new notebook.\n",
    "Dummy Datasets Description\n",
    "Below are four fabricated datasets (each with 20 records). These are intentionally messy: some duplicates, inconsistent spacing, different date formats, etc., to mimic real-world scenarios. Feel free to copy them into local CSV/JSON files or define them in your notebook directly.\n",
    "\n",
    "1. Registrations Dataset (registration.csv)\n",
    "This dataset simulates a car registration table.\n",
    "\n",
    "<details> <summary>Click to expand <code>registration.csv</code> sample data (20 rows)</summary>\n",
    "mathematica\n",
    "Copy\n",
    "Edit\n",
    "REGISTRATION_ID,LICENSE_PLATE,REGISTRATION_STATUS,REGISTRATION_EXPIRY_DATE,LOCATION,COUNTRY_STATE\n",
    "1,ABC123,Active,2025-06-30,Springfield,USA-IL\n",
    "2,XYZ789,Expired,2023-08-15,Shelbyville,USA-IN\n",
    "3,JHG654, Active,06/15/2025,Capital City,USA-IL\n",
    "4,RES321,Inactive,2025/07/01,Shelbyville,USA-IN\n",
    "5,BHN908,Active,2024-12-31,Ogdenville,USA-OH\n",
    "6,ABC123,Active,2025-06-30,Springfield,USA-IL\n",
    "7,QWE111,Expired,15-01-2023,Shelbyville,USA-IN\n",
    "8,ASD222,Active,2025-06-15,Springfield,USA-IL\n",
    "9,ZXC333,Active,2023-06-30,North Haverbrook,USA-MI\n",
    "10,UIO444, Active,2025-06-30,Shelbyville,USA-IN\n",
    "11,JKL555,Inactive,2023/09/01,Capital City,USA-IL\n",
    "12,   MNB666,Expired,2025-06-30,Springfield,USA-IL\n",
    "13,VFR777,Active,2025/06/30,Ogdenville,USA-OH\n",
    "14,PLM999, Active,2022-12-01,Capital City,USA-IL\n",
    "15,OKM432,Inactive,2023-11-20,Shelbyville,USA-IN\n",
    "16,KIU098,Active,2025-07-15,North Haverbrook,USA-MI\n",
    "17,GHF765,Expired,2025-05-01,Ogdenville,USA-OH\n",
    "18,REW111,Active,05-30-2025,Shelbyville,USA-IN\n",
    "19,LKA112,Active,2023-06-30,Shelbyville,USA-IN\n",
    "20,BHN908,Active,2024-12-31,Ogdenville,USA-OH\n",
    "Notable Messiness:\n",
    "\n",
    "LICENSE_PLATE repeated (e.g., ABC123 in rows 1 and 6, BHN908 in rows 5 and 20).\n",
    "Inconsistent spacing in REGISTRATION_STATUS (e.g., Active vs. Active).\n",
    "Different date formats: YYYY-MM-DD, YYYY/MM/DD, MM/DD/YYYY, etc.\n",
    "Extra spaces before MNB666.\n",
    "Duplicated or repeated ID? We see REGISTRATION_ID might be unique, but LICENSE_PLATE can repeat.\n",
    "</details>\n",
    "2. Ownership Dataset (ownership.json)\n",
    "This dataset simulates car ownership details in JSON format.\n",
    "\n",
    "<details> <summary>Click to expand <code>ownership.json</code> sample data (20 rows)</summary>\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "[\n",
    "  {\n",
    "    \"owner_id\": 100,\n",
    "    \"license_plate\": \"ABC123\",\n",
    "    \"owner_name\": \"Homer Simpson\",\n",
    "    \"owner_location\": \"Springfield\",\n",
    "    \"region\": \"USA-IL\",\n",
    "    \"purchase_date\": \"2021-05-20\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 101,\n",
    "    \"license_plate\": \"XYZ789\",\n",
    "    \"owner_name\": \"Marge Simpson\",\n",
    "    \"owner_location\": \"Shelbyville\",\n",
    "    \"region\": \"USA-IN\",\n",
    "    \"purchase_date\": \"2022-08-01\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 102,\n",
    "    \"license_plate\": \"JHG654\",\n",
    "    \"owner_name\": \"Bart Simpson\",\n",
    "    \"owner_location\": \" Capital City \",\n",
    "    \"region\": \"USA-IL\",\n",
    "    \"purchase_date\": \"2023/01/10\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 103,\n",
    "    \"license_plate\": \"RES321\",\n",
    "    \"owner_name\": \"Moe Szyslak\",\n",
    "    \"owner_location\": \"Shelbyville\",\n",
    "    \"region\": \"USA-IN\",\n",
    "    \"purchase_date\": \"2020-12-05\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 104,\n",
    "    \"license_plate\": \"BHN908\",\n",
    "    \"owner_name\": \"Principal Skinner\",\n",
    "    \"owner_location\": \"Ogdenville\",\n",
    "    \"region\": \"USA-OH\",\n",
    "    \"purchase_date\": \"2019-11-30\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 105,\n",
    "    \"license_plate\": \"QWE111\",\n",
    "    \"owner_name\": \"Grampa Simpson\",\n",
    "    \"owner_location\": \"Shelbyville\",\n",
    "    \"region\": \"USA-IN\",\n",
    "    \"purchase_date\": \"2022-06-15\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 106,\n",
    "    \"license_plate\": \"ASD222\",\n",
    "    \"owner_name\": \"Carl Carlson\",\n",
    "    \"owner_location\": \"Springfield\",\n",
    "    \"region\": \"USA-IL\",\n",
    "    \"purchase_date\": \"2021-08-20\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 107,\n",
    "    \"license_plate\": \"ZXC333\",\n",
    "    \"owner_name\": \"Lenny Leonard\",\n",
    "    \"owner_location\": \"North Haverbrook\",\n",
    "    \"region\": \"USA-MI\",\n",
    "    \"purchase_date\": \"2023-01-01\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 108,\n",
    "    \"license_plate\": \"UIO444\",\n",
    "    \"owner_name\": \"Edna Krabappel\",\n",
    "    \"owner_location\": \" Shelbyville \",\n",
    "    \"region\": \"USA-IN\",\n",
    "    \"purchase_date\": \"2020/10/05\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 109,\n",
    "    \"license_plate\": \"JKL555\",\n",
    "    \"owner_name\": \"Ned Flanders\",\n",
    "    \"owner_location\": \"Capital city\",\n",
    "    \"region\": \"USA-IL\",\n",
    "    \"purchase_date\": \"2022-04-10\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 110,\n",
    "    \"license_plate\": \"MNB666\",\n",
    "    \"owner_name\": \"Maude Flanders\",\n",
    "    \"owner_location\": \"Springfield\",\n",
    "    \"region\": \"USA-IL\",\n",
    "    \"purchase_date\": \"2023-07-12\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 111,\n",
    "    \"license_plate\": \"VFR777\",\n",
    "    \"owner_name\": \"Otto Mann\",\n",
    "    \"owner_location\": \"Ogdenville\",\n",
    "    \"region\": \"USA-OH\",\n",
    "    \"purchase_date\": \"2021-01-10\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 112,\n",
    "    \"license_plate\": \"PLM999\",\n",
    "    \"owner_name\": \"Krusty the Clown\",\n",
    "    \"owner_location\": \"Capital City\",\n",
    "    \"region\": \"USA-IL\",\n",
    "    \"purchase_date\": \"2022-12-01\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 113,\n",
    "    \"license_plate\": \"OKM432\",\n",
    "    \"owner_name\": \"Mr. Burns\",\n",
    "    \"owner_location\": \"Shelbyville\",\n",
    "    \"region\": \"USA-IN\",\n",
    "    \"purchase_date\": \"2023-07-07\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 114,\n",
    "    \"license_plate\": \"KIU098\",\n",
    "    \"owner_name\": \"Smithers\",\n",
    "    \"owner_location\": \"North Haverbrook\",\n",
    "    \"region\": \"USA-MI\",\n",
    "    \"purchase_date\": \"2023-02-14\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 115,\n",
    "    \"license_plate\": \"GHF765\",\n",
    "    \"owner_name\": \"Barney Gumble\",\n",
    "    \"owner_location\": \"Ogdenville\",\n",
    "    \"region\": \"USA-OH\",\n",
    "    \"purchase_date\": \"2023-03-03\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 116,\n",
    "    \"license_plate\": \"REW111\",\n",
    "    \"owner_name\": \"Duffman\",\n",
    "    \"owner_location\": \"Shelbyville\",\n",
    "    \"region\": \"USA-IN\",\n",
    "    \"purchase_date\": \"2021-05-20\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 117,\n",
    "    \"license_plate\": \"LKA112\",\n",
    "    \"owner_name\": \"Milhouse Van Houten\",\n",
    "    \"owner_location\": \"Shelbyville\",\n",
    "    \"region\": \"USA-IN\",\n",
    "    \"purchase_date\": \"2023-06-01\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 118,\n",
    "    \"license_plate\": \"BHN908\",\n",
    "    \"owner_name\": \"Comic Book Guy\",\n",
    "    \"owner_location\": \" Ogdenville \",\n",
    "    \"region\": \"USA-OH\",\n",
    "    \"purchase_date\": \"2021-11-11\"\n",
    "  },\n",
    "  {\n",
    "    \"owner_id\": 119,\n",
    "    \"license_plate\": \"ABC123\",\n",
    "    \"owner_name\": \"Clancy Wiggum\",\n",
    "    \"owner_location\": \"Springfield\",\n",
    "    \"region\": \"USA-IL\",\n",
    "    \"purchase_date\": \"2022/01/01\"\n",
    "  }\n",
    "]\n",
    "Notable Messiness:\n",
    "\n",
    "Extra spaces in owner_location.\n",
    "Slightly different city name cases (Capital City vs. Capital City).\n",
    "Date format differences.\n",
    "Some cars appear multiple times (e.g., ABC123 at owner_id 100 and 119).\n",
    "</details>\n",
    "3. Modifications Dataset (modifications.json)\n",
    "Represents car modifications (similar to \"alterations\").\n",
    "\n",
    "<details> <summary>Click to expand <code>modifications.json</code> sample data (20 rows)</summary>\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "[\n",
    "  {\n",
    "    \"mod_id\": 1,\n",
    "    \"license_plate\": \"ABC123\",\n",
    "    \"modification_type\": \"Engine Upgrade\",\n",
    "    \"status_of_mod_request\": \"Approved\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 2,\n",
    "    \"license_plate\": \"ABC123\",\n",
    "    \"modification_type\": \"Paint Job\",\n",
    "    \"status_of_mod_request\": \"Approved\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 3,\n",
    "    \"license_plate\": \"JHG654\",\n",
    "    \"modification_type\": \"Window Tint\",\n",
    "    \"status_of_mod_request\": \"Pending\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 4,\n",
    "    \"license_plate\": \"ZXC333\",\n",
    "    \"modification_type\": \"Audio System\",\n",
    "    \"status_of_mod_request\": \"Rejected\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 5,\n",
    "    \"license_plate\": \"QWE111\",\n",
    "    \"modification_type\": \"Engine Upgrade\",\n",
    "    \"status_of_mod_request\": \"Approved\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 6,\n",
    "    \"license_plate\": \"RES321\",\n",
    "    \"modification_type\": \"Spoiler\",\n",
    "    \"status_of_mod_request\": \"Approved\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 7,\n",
    "    \"license_plate\": \"XYZ789\",\n",
    "    \"modification_type\": \"Paint Job\",\n",
    "    \"status_of_mod_request\": \"Approved\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 8,\n",
    "    \"license_plate\": \"XYZ789\",\n",
    "    \"modification_type\": \"Engine Upgrade\",\n",
    "    \"status_of_mod_request\": \"Approved\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 9,\n",
    "    \"license_plate\": \"BHN908\",\n",
    "    \"modification_type\": \"Engine Upgrade\",\n",
    "    \"status_of_mod_request\": \"Pending\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 10,\n",
    "    \"license_plate\": \"BHN908\",\n",
    "    \"modification_type\": \"Paint Job\",\n",
    "    \"status_of_mod_request\": \"Pending\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 11,\n",
    "    \"license_plate\": \"BHN908\",\n",
    "    \"modification_type\": \"Window Tint\",\n",
    "    \"status_of_mod_request\": \"Rejected\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 12,\n",
    "    \"license_plate\": \"BHN908\",\n",
    "    \"modification_type\": \"Engine Upgrade\",\n",
    "    \"status_of_mod_request\": \"Approved\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 13,\n",
    "    \"license_plate\": \"BHN908\",\n",
    "    \"modification_type\": \"New Rims\",\n",
    "    \"status_of_mod_request\": \"Approved\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 14,\n",
    "    \"license_plate\": \"UIO444\",\n",
    "    \"modification_type\": \"Audio System\",\n",
    "    \"status_of_mod_request\": \"Pending\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 15,\n",
    "    \"license_plate\": \"PLM999\",\n",
    "    \"modification_type\": \"Paint Job\",\n",
    "    \"status_of_mod_request\": \"Approved\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 16,\n",
    "    \"license_plate\": \"GHF765\",\n",
    "    \"modification_type\": \"Engine Upgrade\",\n",
    "    \"status_of_mod_request\": \"Approved\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 17,\n",
    "    \"license_plate\": \"GHF765\",\n",
    "    \"modification_type\": \"Spoiler\",\n",
    "    \"status_of_mod_request\": \"Pending\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 18,\n",
    "    \"license_plate\": \"GHF765\",\n",
    "    \"modification_type\": \"Window Tint\",\n",
    "    \"status_of_mod_request\": \"Approved\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 19,\n",
    "    \"license_plate\": \"MNB666\",\n",
    "    \"modification_type\": \"New Rims\",\n",
    "    \"status_of_mod_request\": \"Rejected\"\n",
    "  },\n",
    "  {\n",
    "    \"mod_id\": 20,\n",
    "    \"license_plate\": \"MNB666\",\n",
    "    \"modification_type\": \"Spoiler\",\n",
    "    \"status_of_mod_request\": \"Pending\"\n",
    "  }\n",
    "]\n",
    "Notable Points:\n",
    "\n",
    "Some cars have multiple modifications (e.g., BHN908 has 5 modifications).\n",
    "Different categories for modification_type and status_of_mod_request.\n",
    "</details>\n",
    "4. Testing (Inspection) Dataset (testing.csv)\n",
    "Simulates a dataset of car testing/inspection results.\n",
    "\n",
    "<details> <summary>Click to expand <code>testing.csv</code> sample data (20 rows)</summary>\n",
    "mathematica\n",
    "Copy\n",
    "Edit\n",
    "inspection_id,license_plate,inspection_date,inspection_outcome\n",
    "1,ABC123,2021-06-01,Pass\n",
    "2,ABC123,2022-06-02,Pass\n",
    "3,XYZ789,2022-08-16,Fail\n",
    "4,JHG654,2023-01-11,Pass\n",
    "5,QWE111,2022-06-15,Fail\n",
    "6,ZXC333,2023-02-01,Incomplete\n",
    "7,ZXC333,2023-06-30,Pass\n",
    "8,BHN908,2020-12-31,Fail\n",
    "9,BHN908,2021-12-31,Fail\n",
    "10,BHN908,2023-01-15,Pass\n",
    "11,BHN908,2023-06-01,   pass\n",
    "12,UIO444,2022-10-06,Fail\n",
    "13,JKL555,2023-09-02,FAIL\n",
    "14,MNB666,2025-07-01,Pass\n",
    "15,VFR777,2024-01-10,Pass\n",
    "16,PLM999,2023-12-01,Partial\n",
    "17,KIU098,2025-07-15,Pass\n",
    "18,GHF765,2025-05-15,fail\n",
    "19,REW111,2023-05-30,Unknown\n",
    "20,LKA112,2023-06-30,Pass\n",
    "Notable Messiness:\n",
    "\n",
    "Different ways of writing pass/fail (Pass, pass, FAIL, fail, etc.).\n",
    "“Incomplete”, “Partial”, “Unknown” categories with few observations.\n",
    "</details>\n",
    "Working Through the ETL Steps\n",
    "Below is a Jupyter Notebook style walk-through. Please copy the snippets into your notebook cells. Run them incrementally to follow along.\n",
    "\n",
    "Data Extraction: Loading the Data into Pandas\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To display plots inline in Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# 1) Read the registration.csv file\n",
    "registration_df = pd.read_csv('registration.csv')\n",
    "\n",
    "# 2) Read the ownership.json file\n",
    "ownership_df = pd.read_json('ownership.json')\n",
    "\n",
    "# 3) Read the modifications.json file\n",
    "modifications_df = pd.read_json('modifications.json')\n",
    "\n",
    "# 4) Read the testing.csv file\n",
    "testing_df = pd.read_csv('testing.csv')\n",
    "\n",
    "# Let's do a quick look at each DataFrame\n",
    "print(\"=== Registrations ===\")\n",
    "display(registration_df.head())\n",
    "\n",
    "print(\"=== Ownership ===\")\n",
    "display(ownership_df.head())\n",
    "\n",
    "print(\"=== Modifications ===\")\n",
    "display(modifications_df.head())\n",
    "\n",
    "print(\"=== Testing ===\")\n",
    "display(testing_df.head())\n",
    "At this point, we’ve extracted the data. Next, we’ll transform and analyze step-by-step.\n",
    "\n",
    "Data Exploration & Transformation\n",
    "Let’s mirror the requirements from the original outline—just adapted to our new car context.\n",
    "\n",
    "Part 1 – Analysis of Registrations (registration.csv)\n",
    "1a) Identify Unique Identifier\n",
    "Goal: We need to figure out which column can uniquely identify a car.\n",
    "\n",
    "In the elevator scenario, we used the “elevator unique ID.” Here, it seems we can use LICENSE_PLATE to consistently track a car across datasets. However, let’s confirm by scanning columns using code.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "print(\"Columns:\", registration_df.columns.tolist())\n",
    "\n",
    "# We'll check the cardinality of each column\n",
    "for col in registration_df.columns:\n",
    "    print(f\"{col} has {registration_df[col].nunique()} unique values.\")\n",
    "You might see:\n",
    "\n",
    "REGISTRATION_ID: 20 unique values (but watch out for duplication possibility).\n",
    "LICENSE_PLATE: fewer or more unique values (some duplicates).\n",
    "In real-world terms, LICENSE_PLATE identifies the car. Even if we see duplicates in registration.csv, it’s presumably the same car re-registered or repeated rows. For the sake of merging across data, we choose LICENSE_PLATE as the anchor.\n",
    "\n",
    "1b) Majority Location & Extract Country/State\n",
    "Goal: We want to see which location is most frequent, and split the COUNTRY_STATE into two new columns: COUNTRY and STATE.\n",
    "\n",
    "First, find the majority location:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "registration_df['LOCATION'].value_counts()\n",
    "This shows us which city is most common.\n",
    "\n",
    "Then, extract COUNTRY and STATE from COUNTRY_STATE, which looks like USA-IL, USA-IN, etc. Pandas has a handy string method for splitting:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Split the COUNTRY_STATE column\n",
    "registration_df[['COUNTRY', 'STATE']] = registration_df['COUNTRY_STATE'].str.split('-', expand=True)\n",
    "\n",
    "# Let's verify\n",
    "registration_df.head()\n",
    "1c) Filter the DataFrame on Registration Status\n",
    "Goal: Decide how to filter REGISTRATION_STATUS. Possibly we only want Active statuses and remove expired/inactive.\n",
    "\n",
    "Check unique statuses:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "registration_df['REGISTRATION_STATUS'].unique()\n",
    "You’ll see values like: ['Active', 'Expired', 'Inactive', ' Active', ...] with inconsistent spacing.\n",
    "\n",
    "Step 1: Clean spacing:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "registration_df['REGISTRATION_STATUS'] = registration_df['REGISTRATION_STATUS'].str.strip()\n",
    "Step 2: Filter to keep only 'Active' or maybe 'Active' + 'Inactive' depending on your rationale. Suppose we keep only 'Active':\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "registration_df = registration_df[registration_df['REGISTRATION_STATUS'] == 'Active']\n",
    "\n",
    "# Overwrite the DataFrame\n",
    "registration_df.head()\n",
    "1d) Verify Uniqueness of the Identifier\n",
    "Goal: Double-check LICENSE_PLATE is unique.\n",
    "\n",
    "We suspect it might not be strictly unique because the same plate can appear in multiple rows if the data is messy. Let’s check:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "print(\"Number of rows:\", len(registration_df))\n",
    "print(\"Number of unique LICENSE_PLATE:\", registration_df['LICENSE_PLATE'].nunique())\n",
    "\n",
    "duplicates = registration_df[registration_df.duplicated('LICENSE_PLATE', keep=False)]\n",
    "display(duplicates)\n",
    "You’ll see duplicates for certain plates. This means LICENSE_PLATE is not a strict unique key in this table. It’s still the best cross-dataset identifier we have, but be mindful we can have multiple rows per plate here.\n",
    "\n",
    "1e) Time Series of Expiration Counts by Month\n",
    "Goal: Plot a time series for REGISTRATION_EXPIRY_DATE counts by month. Avoid hard-coding months.\n",
    "\n",
    "We must parse the date column first because of multiple date formats. Let’s do a robust parse:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Convert to datetime (coerce errors to NaT, which we might drop or fix later)\n",
    "registration_df['REGISTRATION_EXPIRY_DATE'] = pd.to_datetime(\n",
    "    registration_df['REGISTRATION_EXPIRY_DATE'], errors='coerce', infer_datetime_format=True\n",
    ")\n",
    "\n",
    "# Now let's extract year-month\n",
    "registration_df['expiry_year_month'] = registration_df['REGISTRATION_EXPIRY_DATE'].dt.to_period('M')\n",
    "\n",
    "# Count how many expirations per year-month\n",
    "expiration_counts = registration_df['expiry_year_month'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "expiration_counts.plot(kind='line', marker='o')\n",
    "plt.title(\"Number of Expirations by Year-Month\")\n",
    "plt.xlabel(\"Year-Month\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "1f) Create a Year-Month Table with Counts\n",
    "Goal: Similar to above but produce a table. Then add a more readable format like January 2025.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Re-use our 'expiry_year_month' column\n",
    "ym_counts = registration_df.groupby('expiry_year_month').size().reset_index(name='count')\n",
    "\n",
    "# Filter only year-months with more than 5 occurrences\n",
    "ym_counts = ym_counts[ym_counts['count'] > 5]\n",
    "\n",
    "# Add a new column with format 'Month Year'\n",
    "# Convert period to timestamp, then format\n",
    "ym_counts['readable_month_year'] = ym_counts['expiry_year_month'].dt.strftime('%B %Y')\n",
    "\n",
    "ym_counts\n",
    "That yields a table of year-month combos (only those with > 5 occurrences).\n",
    "\n",
    "Part 2 – Analysis of Ownership (ownership.json)\n",
    "We have our filtered registrations from Part 1. Let’s merge them with the ownership data.\n",
    "\n",
    "2a) Merge Filtered Registrations with Ownership\n",
    "Goal: Join the two DataFrames on LICENSE_PLATE.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "merged_reg_own = pd.merge(registration_df, ownership_df, \n",
    "                          on='license_plate', \n",
    "                          how='inner',  # or 'left', depending on your need\n",
    "                          suffixes=('_reg','_own'))\n",
    "\n",
    "print(\"Rows from registration_df (filtered):\", len(registration_df))\n",
    "print(\"Rows from ownership_df:\", len(ownership_df))\n",
    "print(\"Rows in merged dataset:\", len(merged_reg_own))\n",
    "You should see how many rows were matched.\n",
    "\n",
    "2b) Compare Location Fields\n",
    "Goal: We might have LOCATION from registrations vs. owner_location from ownership. Let’s see differences.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "mismatch = merged_reg_own[merged_reg_own['LOCATION'] != merged_reg_own['owner_location'].str.strip()]\n",
    "mismatch[['license_plate','LOCATION','owner_location']]\n",
    "Note any discrepancies such as extra spaces, different city names, or capitalizations.\n",
    "\n",
    "2c) Further Location Cleanup/Filtering\n",
    "Goal: Decide how to handle these mismatches. Maybe we want to keep only rows where LOCATION and owner_location match after a bit of cleaning.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Clean up owner_location\n",
    "merged_reg_own['owner_location'] = merged_reg_own['owner_location'].str.strip().str.lower()\n",
    "merged_reg_own['LOCATION'] = merged_reg_own['LOCATION'].str.strip().str.lower()\n",
    "\n",
    "# Now filter\n",
    "merged_reg_own = merged_reg_own[merged_reg_own['LOCATION'] == merged_reg_own['owner_location']]\n",
    "Now we only keep data where the registration city matches the owner’s city (after lowercasing/spaces removed).\n",
    "\n",
    "2d) Identify a Variable to Filter the Dataset\n",
    "Goal: We might want only records in certain region(s) or a certain purchase date range.\n",
    "\n",
    "For instance, we can filter out owners who purchased before 2021:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Convert purchase_date to datetime\n",
    "merged_reg_own['purchase_date'] = pd.to_datetime(merged_reg_own['purchase_date'], errors='coerce')\n",
    "filtered_merged = merged_reg_own[merged_reg_own['purchase_date'] >= '2021-01-01']\n",
    "\n",
    "# Overwrite\n",
    "merged_reg_own = filtered_merged\n",
    "2e) Clean or Reduce Categories of a Variable\n",
    "Goal: Identify a column with more than 5 categories and unify or reduce them.\n",
    "\n",
    "Possibilities:\n",
    "\n",
    "region might have multiple states (USA-IL, USA-IN, USA-OH, USA-MI).\n",
    "owner_location can have multiple city names.\n",
    "If we pick region, we see these categories:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "merged_reg_own['region'].value_counts()\n",
    "Let’s say we have 4 states, but we want to group them into fewer (like grouping USA-IL and USA-IN together). An example:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def simplify_region(r):\n",
    "    if r in ['USA-IL','USA-IN']:\n",
    "        return 'Midwest_A'\n",
    "    elif r in ['USA-OH','USA-MI']:\n",
    "        return 'Midwest_B'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "merged_reg_own['region_simplified'] = merged_reg_own['region'].apply(simplify_region)\n",
    "Now we have fewer categories.\n",
    "\n",
    "Part 3 – Analysis of Modifications (modifications.json)\n",
    "3a) Merge with the Already Merged Data\n",
    "Goal: Bring in modifications so we know which cars had modifications.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "merged_reg_own_mod = pd.merge(merged_reg_own, modifications_df,\n",
    "                              on='license_plate',\n",
    "                              how='left')\n",
    "Using a how='left' ensures we keep all records from the current merged data, even if they have no modifications (which will appear as NaN in mod columns).\n",
    "\n",
    "3b) Find Cars with More than 5 Modifications\n",
    "Goal: Some cars might have multiple modification entries. Let’s identify them.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Group by license_plate, count modifications\n",
    "mod_count = merged_reg_own_mod.groupby('license_plate')['mod_id'].count().reset_index(name='mod_count')\n",
    "\n",
    "# Filter\n",
    "cars_with_5plus = mod_count[mod_count['mod_count'] > 5]\n",
    "cars_with_5plus\n",
    "Then we can see which cars appear:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Display rows related to those cars\n",
    "merged_reg_own_mod[merged_reg_own_mod['license_plate'].isin(cars_with_5plus['license_plate'])]\n",
    "3c) Count Combinations of Modification Type and Status\n",
    "Goal: Count each unique combination, e.g., “Engine Upgrade”-“Approved”.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "combos = merged_reg_own_mod.groupby(['modification_type','status_of_mod_request']).size().reset_index(name='count')\n",
    "combos\n",
    "We can see how many times each type + status pairing appears.\n",
    "\n",
    "Part 4 – Analysis of Testing (testing.csv)\n",
    "4a) Merge Strategy to Avoid Many-to-Many Issues\n",
    "Goal: We now merge the (Registration-Ownership-Modifications) DataFrame with testing_df.\n",
    "\n",
    "We must watch out for many duplicates. If one car has multiple modifications and multiple inspections, a naive merge could produce a “grid” of duplicates.\n",
    "One strategy: Merge on a subset (e.g., license_plate) carefully, or reduce duplicates first. For example, if we only need the first or last modification, we can group or drop duplicates:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# For demonstration, let's keep the first modification entry per license_plate\n",
    "reduced_mod = merged_reg_own_mod.drop_duplicates(subset=['license_plate'])\n",
    "\n",
    "# Now merge with testing\n",
    "final_merged = pd.merge(reduced_mod, testing_df, on='license_plate', how='left')\n",
    "\n",
    "print(\"Rows in reduced_mod:\", len(reduced_mod))\n",
    "print(\"Rows in testing_df:\", len(testing_df))\n",
    "print(\"Rows in final_merged:\", len(final_merged))\n",
    "Explain your choice in real scenarios. Alternatively, you can keep the full data but be aware of the repeated rows.\n",
    "\n",
    "4b) Clean the Inspection Outcome Variable\n",
    "Goal: Unify categories: 'Pass', 'Fail', 'Incomplete', 'Partial', 'Unknown', etc. For categories with few occurrences, merge into 'Other'.\n",
    "\n",
    "First, let’s strip and lower everything:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "final_merged['inspection_outcome'] = final_merged['inspection_outcome'].str.strip().str.lower()\n",
    "print(final_merged['inspection_outcome'].value_counts(dropna=False))\n",
    "You might see:\n",
    "\n",
    "pass, fail, incomplete, partial, unknown, plus weird spacing.\n",
    "Combine smaller categories:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Let's see total counts\n",
    "outcome_counts = final_merged['inspection_outcome'].value_counts()\n",
    "threshold = 2  # e.g., if the category has <=2 occurrences, we'll merge into \"other\"\n",
    "\n",
    "def unify_outcome(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    elif outcome_counts[x] <= threshold:\n",
    "        return 'other'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "final_merged['inspection_outcome_clean'] = final_merged['inspection_outcome'].apply(unify_outcome)\n",
    "final_merged['inspection_outcome_clean'].value_counts(dropna=False)\n",
    "We might also unify 'fail', 'FAIL', 'fail ' → 'fail'; ' pass' → 'pass'; etc.\n",
    "\n",
    "4c) Calculate the Average Number of Inspections per Car\n",
    "Goal: Each car can have multiple inspection rows. Let’s find the average.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "inspections_count = testing_df.groupby('license_plate')['inspection_id'].count().reset_index(name='inspection_count')\n",
    "avg_inspections = inspections_count['inspection_count'].mean()\n",
    "print(\"Average number of inspections per car:\", avg_inspections)\n",
    "Conclusion\n",
    "Congratulations! You’ve walked through a miniature ETL project:\n",
    "\n",
    "Extract: Loaded CSV and JSON into Pandas.\n",
    "Transform: Cleaned messy data (spaces, date formats), filtered rows, merged multiple datasets, reduced categories, and handled duplicates.\n",
    "Load/Analyze: Did some basic analyses—time series plots, frequency counts, aggregator tables—to glean insights.\n",
    "In your real elevator project, you’ll follow the same steps but with actual data. The main difference is scale and domain complexity. The logic—unique identifier selection, data cleaning, merges, filtering, grouping, and simple visualizations—all remain the same.\n",
    "\n",
    "Key Takeaways:\n",
    "\n",
    "Always explore data with .head(), .info(), .describe(), .unique(), and .value_counts().\n",
    "Clean thoroughly: trim strings, unify date formats, handle duplicates.\n",
    "Merging large datasets often requires careful strategies to avoid memory blowups.\n",
    "Summaries and visuals (Matplotlib, Seaborn) help communicate your findings.\n",
    "Feel free to experiment further with these dummy datasets. For instance:\n",
    "\n",
    "Try different merges (inner, outer, left, right).\n",
    "Attempt a more complex cleaning strategy for the city/region columns.\n",
    "Use additional libraries like Seaborn for advanced plots.\n",
    "Good luck, and enjoy your ETL journey!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
