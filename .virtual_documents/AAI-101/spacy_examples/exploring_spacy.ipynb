














import spacy  # Import the spaCy library



# Load the small English model. We use a descriptive variable name 'nlp' (short for Natural Language Processing)
nlp = spacy.load("en_core_web_sm")


# Process a sample text using the loaded model.
doc: spacy.tokens.Doc = nlp("spaCy is an amazing NLP library!")


# Let's print out each token (word or punctuation) in our processed document.
print("Tokens:", [token.text for token in doc])








# Let's create a new document from a different sentence.
doc1 = nlp("spaCy is great for various NLP tasks!")


# Print each token. Notice that punctuation marks are treated as separate tokens.
tokens = [token.text for token in doc1]
print("Tokens:", tokens)








# Loop through each token in our document and print its POS information.
for token in doc1:
    # token.pos_ gives the simplified POS tag and token.tag_ gives the detailed tag.
    print(f"{token.text:10} -> {token.pos_:6} ({token.tag_})")








# Sample text with multiple entities.
text = "Apple was founded by Steve Jobs in California."
doc2 = nlp(text)


# Loop through the entities recognized in the text.
for ent in doc2.ents:
    # spacy.explain() provides a friendly explanation for the entity label.
    print(f"{ent.text:12} -> {ent.label_:6} ({spacy.explain(ent.label_)})")








# Let's examine the dependency structure of our original doc.
for token in doc:
    # Each token is printed along with its dependency relation and the text of its head (the word it is connected to).
    print(f"{token.text:10} -> {token.dep_:10} (head: {token.head.text})")








# Display each token alongside its lemma.
print("Original vs. Lemma:")
for token in doc:
    print(f"{token.text:10} -> {token.lemma_}")








from spacy.lang.en.stop_words import STOP_WORDS

# Filter out tokens that are stopwords.
filtered_tokens = [token.text for token in doc if not token.is_stop]

print("Original text:", doc.text)
print("Filtered tokens (without stopwords):", filtered_tokens)








from spacy.pipeline import EntityRuler

# Reload the English model (or use your existing one).
nlp = spacy.load("en_core_web_sm")


# Add the EntityRuler to the pipeline before the built-in NER component.
ruler = nlp.add_pipe("entity_ruler", before="ner")


# Define custom patterns. Here, we define "spaCy" as a SOFTWARE entity.
patterns = [{"label": "SOFTWARE", "pattern": "spaCy"}]
ruler.add_patterns(patterns)


# Process a sentence to see our custom entity in action.
doc_custom = nlp("spaCy is a great NLP tool.")
print("Custom Entities:", [(ent.text, ent.label_) for ent in doc_custom.ents])








# Save the current spaCy pipeline to disk.
nlp.to_disk("my_spacy_model")



# Later, load your custom model from disk.
nlp_custom = spacy.load("my_spacy_model")








import json  # Import the JSON library for formatting output



# Create a list of dictionaries for each token in doc2.
export_data = [
    {"text": token.text, "lemma": token.lemma_, "pos": token.pos_}
    for token in doc2
]


# Pretty-print the exported data as JSON.
print("Exported Data:")
print(json.dumps(export_data, indent=2))








from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Choose some sample text.
sample_text = (
    "Natural Language Processing is fun and exciting. "
    "SpaCy makes it simple to build real-world NLP applications. "
    "Learning NLP with spaCy is a great way to get started with artificial intelligence."
)


# Create a WordCloud object.
# We use f-strings and keyword arguments to customize the appearance.
wordcloud = WordCloud(width=800, height=400, background_color="white", colormap="viridis").generate(sample_text)


# Display the generated word cloud using matplotlib.
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")  # Turn off the axis lines and labels
plt.title("Word Cloud of Sample Text", fontsize=16)
plt.show()









